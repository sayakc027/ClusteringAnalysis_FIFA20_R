---
title: <center> <b> Unsupervised Learning - FIFA 20 </b> </center>
subtitle: <center> <b> Cluster Analysis on EA FIFA 20 Data Set </b> </center>
author: <center> Sayak Chakraborty | Samreen Zehra | Niharika Gupta | Rohit Thakur </center>
output: word_document
---

<style>
body {
        text-align: justify;
        font-family: "Bookman", Bookman;
        font-size: 14pt;
     }
</style>

<style>

h1 {
  color: #0002C0;
  font-family: Bookman;
  font-weight:bold;
}

h2 {
  color: #0002C0;
  font-family: Bookman;
font-weight:bold;
}

h3 {
  color: #0002C0;
  font-family: Bookman;
font-weight:bold;
}

h4 {
  color: #0002C0;
  font-family: Bookman;
font-weight:bold;
}

a, a:hover {
    color: #C70039 ;
}

</style>

# {.tabset .tabset-fade .tabset-pills}

## **Introduction**

<h1> <b> FIFA </b> </h1>

FIFA, also known as FIFA Football or FIFA Soccer, is a series of association football video games or football simulator, released annually by Electronic Arts under the EA Sports label. Football video games such as Sensible Soccer, Kick Off and Match Day had been developed since the late 1980s and already competitive in the games market when EA Sports announced a football game as the next addition to their EA Sports label. The Guardian called the series "the slickest, most polished and by far the most popular football game around. 

* As of 2011, the FIFA franchise has been localised into 18 languages and available in 51 countries.
* Listed in Guinness World Records as the best-selling sports video game franchise in the world, by 2018, the FIFA series had sold over 260 million copies. 
* It is also one of the best-selling video game franchises.

Source: [**Wikipedia**](https://en.wikipedia.org/wiki/FIFA_(video_game_series))

<hr>

Data Source: [**Kaggle FIFA 20 Data Set**](https://www.kaggle.com/stefanoleone992/fifa-20-complete-player-dataset)

<hr>

</br>


![](FIFA 20 Image top 3.jpg)

<hr>

</br>

<h1> <b> Problem Statement </b> </h1>

In professional football, it is not uncommon for a player to drop out of team due to contract transfer, contract expiration or medical reasons. By the means of our analysis, we will be trying to find suitable replacements players based on their skillset, strength and physical attributes. We will be doing that by using Clustering Analysis which is an unsupervised learning method. We will be building two different models using K-means clustering and Hierarchical clustering and in the end compare both the models to see which one performs better.

</br>

## **Data Preparation**

<h3> Packages Used </h3>

We begin by loading the packages that will be required throughout the course of our analysis.

```{r Loading the required packages, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

library (tidyverse) 
library (ggfortify)
library (qdapTools)
library (caret)
library (tidyr)
library (corrplot)
library (ggpubr)
library(ggplot2)
library(PerformanceAnalytics)
library(factoextra)
library(fpc)

```

<hr>

</br>

<h3> Importing the Data </h3>

The data set was imported into R studio using the below code

```{r read the data, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
fifa_data <- read.csv("fifa-20-complete-player-dataset/players_20.csv", stringsAsFactors = FALSE)

attach(fifa_data)

set.seed(123456)
```

</br>

Displaying the Top 10 Values of the Data 

```{r Display top 10 data, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

head(fifa_data, 10)

```

</br>

<h3> Structure of the data </h3>

Next we use the str() function to see the structure of the dataset.

```{r structure of the data, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

str(fifa_data)

```

<h4> Observations: </h4>

The dataset has 18278 observations and 104 variables.
The dataset has both numeric and character variables.

</br>

<h3> Glimpse of the data </h3>

```{r Glimpse of the data, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

glimpse(fifa_data)

```

<hr>

</br>

<h1> <b> Data Cleaning - Splitting into 2 groups </b> </h1>
<h1> <b> Goalkeepers and Outflied Players </b> </h1>

Both goalkeepers and outfield players have different set of attributes. It makes more sense to divide the whole dataset into two groups: Outfield players and Goal keepers so that there are no Null values as we can assign only the attributes that are relevant to each group. This will also help us to align our model to our problem statement.

<hr>

</br>

<h3> Data Set - Goalkeepers </h3>

```{r Goalkeepers, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
data_gk <- subset(x=fifa_data, subset= player_positions == "GK", select = c(sofifa_id,short_name,age,height_cm,weight_kg,overall,potential,value_eur,wage_eur,player_positions,international_reputation,weak_foot,skill_moves,gk_diving,gk_handling,gk_kicking,gk_reflexes,gk_speed,gk_positioning,attacking_crossing,attacking_finishing,attacking_heading_accuracy,attacking_short_passing,attacking_volleys,skill_dribbling,skill_curve,skill_fk_accuracy,skill_long_passing,skill_ball_control,movement_acceleration,movement_sprint_speed,movement_agility,movement_reactions,movement_balance,power_shot_power,power_jumping,power_stamina,power_strength,power_long_shots,mentality_aggression,mentality_interceptions,mentality_positioning,mentality_vision,mentality_penalties,mentality_composure,defending_marking,defending_standing_tackle,defending_sliding_tackle,goalkeeping_diving,goalkeeping_handling,goalkeeping_kicking,goalkeeping_positioning,goalkeeping_reflexes))

```

</br>

<h3> Analysing the Goalkeeper Dataset - Head, Dimension and Glimpse </h3>

```{r top 10 goalkeeper dataset, results='hide'}
head(data_gk)
dim(data_gk)
glimpse(data_gk)

```

<hr>

</br>

<h4> Observations: </h4>

The goalkeeper dataset has 2036 observations and 53 variables.

<h3> Outfield Players Dataset </h3>

```{r Outfield Players Dataset,}
data_outfield <- subset(x=fifa_data, subset= player_positions != "GK", select = c(sofifa_id,short_name,age,height_cm,weight_kg,overall,potential,value_eur,wage_eur,player_positions,international_reputation,weak_foot,skill_moves,pace,shooting,passing,dribbling,defending,physic,attacking_crossing,attacking_finishing,attacking_heading_accuracy,attacking_short_passing,attacking_volleys,skill_dribbling,skill_curve,skill_fk_accuracy,skill_long_passing,skill_ball_control,movement_acceleration,movement_sprint_speed,movement_agility,movement_reactions,movement_balance,power_shot_power,power_jumping,power_stamina,power_strength,power_long_shots,mentality_aggression,mentality_interceptions,mentality_positioning,mentality_vision,mentality_penalties,mentality_composure,defending_marking,defending_standing_tackle,defending_sliding_tackle,goalkeeping_diving,goalkeeping_handling,goalkeeping_kicking,goalkeeping_positioning,goalkeeping_reflexes))

```


<h3> Analysing the Outfield Dataset - Head, Dimension and Glimpse </h3>

```{r Head and Dim of Outfield Players, results='hide'}
head(data_outfield)
dim(data_outfield)
glimpse(data_outfield)
```

<h4> Observations: </h4>

The Outfield dataset has <b>16242</b> observations and <b>53</b> variables.

<hr>

</br>

<h2> Creating Data Sets for Goalkeepers and Outfield Players by only taking Physical, Strength and Skill Attritubes for Clustering </h2>

* Since we are using physical attributes, strength and skillset to find suitable replacement players, we will be filtering the datasets to retain only these variables.

* The resulting datasets have a total of 47 variables each.

```{r clustering datasets, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

data_gk_int <- subset(data_gk,select = -c(short_name, player_positions, sofifa_id, value_eur, wage_eur, skill_moves))

glimpse (data_gk_int)

data_outfield_int <- subset(data_outfield, select = -c(short_name, player_positions, sofifa_id, value_eur, wage_eur, skill_moves))

glimpse(data_outfield_int)

```

<hr>

</br>

<h3> Scaling the Dataframes </h2>

Next as a part of Data Pre Processing we use the scale() function to basically normalize the datasets. This function places continuous variables on unit scale by subtracting the mean of the variable and dividing the result by the variableâ€™s standard deviation (also sometimes called z-scoring or simply scaling). The result is that the values in the transformed variable have the same relationship to one another as in the untransformed variable, but the transformed variable has mean 0 and standard deviation.

```{r Scaling the Dataframes, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

data_gk_int <- scale(data_gk_int)

data_outfield_int <- scale(data_outfield_int)

set.seed(123456)
```


## **EDA**

<h3>Exploratory Data Analysis</h3>
Sometimes same player takes over several positions, sometimes only one like a goal keeper. Also notice that names of positions separated by comma and space if several are present. What we can do here is to display for each player it's relative relation to a position. This can be accomplished in few steps.

* We remove white spaces from all cells in player_positions column.
* We separate any string in the cell by comma if there any comma present.
* We transform column into a matrix N x n where N - number of rows in the data (total number of rows present in the table) and n is the total number of unique attributes found in the column. 

In our case we have 15 attributes, so the table will be N x 15.

```{r Players Positions, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center' }

print ("Players position")
head (player_positions)

player_positions <- gsub('\\s+', '',player_positions)#remove white spaces from specified columns
players_role <- mtabulate(strsplit(player_positions, ",")) #one hot encode player position

print ("Number of entries (rows) and number of (columns) of the matrix")
dim (players_role)
```

<h3> Frquency of Positions Played </h3>

```{r Frequency of players played, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'  }
role_frequency <- t(sort(apply (players_role, 2, sum))) #count number of players position appearing

role_frequency <- gather (data.frame(role_frequency), Position, Count) #transfer data into format for ggplot

role_frequency$Position <- reorder(role_frequency$Position, role_frequency$Count)#reorder position by ascending count

#barplot of players position frequency

ggplot(role_frequency, aes(x=Position, y = Count, fill = Position)) + 
  geom_bar(stat = "identity", color ="black", fill = "#B03A2E") + 
  coord_flip() +
  theme_minimal() +
  geom_text(aes(label = Count, hjust = 1, color = "#FDFEFE" , size = 3.5)) +
  ggtitle("Frequency of Positions Played") +
  xlab("Player Positions") + 
  ylab("Count") +
  theme(legend.position = "none",
        plot.title = element_text(color = "black", size = 14, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(color = "darkblue", hjust = 0.5),
        axis.title.y = element_text(),
        axis.title.x = element_text(),
        axis.ticks = element_blank())

  

```



<hr>
</br>

<h3> Correlation amongst outfield players </h3>

To see correlation between values in the table (players_role) we can compute correlation between them by applying function cor. It requires only numeric columns, make sure you do not have any rows with string inside.

We can plot corrplot using corrplot function of corrplot package. One can specify various arguments of a function. corrplot:: specifies the namespace of a package. It can be helpful when two packages have different function but share same function name.

```{r Position Correalation, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'  }

role_cor <- cor(players_role) #correlation between players role

corrplot::corrplot(role_cor, hclust.method = "ward.D", order = "hclust", method = "square", diag = F, type = "lower" ) #correlation plot

```

<h4> Observation: </h4>
We can see that mid center players often share position of a center defending midfielder and less with center attacking midfielder. Left and right back wingers clearly have positive correlation so as right and left mid-siders.

It seems that central forward position (ST) is negatively correlated with (CM - Central middle, CDM - Central defense middle, CB - Central backfielder, RB - right backfielder) a little less with LB - left backfielder. Clearly players playing in the forward position rarely play in the defense


<h3> Correrlation of key statistics from Outfield dataframes: </h3>

```{r Skill Correlation, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center' }
data_outfield_Summary <- subset(x=data_outfield, select = c(age,height_cm,weight_kg, overall, potential,pace,shooting,passing,dribbling,defending,physic))

corr1 <- cor(data_outfield_Summary)
corrplot::corrplot(corr1, hclust.method = "ward.D", order = "hclust", method = "square",diag = F, type = "lower" ) #correlation plot for Outfield Players


#chart.Correlation(data_outfield_Summary, histogram=TRUE, pch=19)

```




## **Cluster Analysis** {.tabset .tabset-fade .tabset-pills}

<h3> Definition </h3>
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.

More Information can be found on [**Wikipedia**](https://en.wikipedia.org/wiki/Cluster_analysis)

</br>

### **K-Means Clustering**

<h3> Definition </h3>
K-Means Clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids.

</br>

More Information can be found on [**Wikipedia**](https://en.wikipedia.org/wiki/K-means_clustering)


</br>


We want to view the result so We can use ```fviz_cluster```. It is function can provide a nice graph of the clusters. Usually, we have more than two dimensions (variables) ```fviz_cluster will perform principal component analysis (PCA)``` and plot the data points according to the first two principal components that explain the majority of the variance.


</br>

<h2> Cluster Analysis for Goalkeepers </h2>

In order to use k-means method for clustering and plot results, we can use k means function in R. It will group the data into a specified number of clusters (centers = 2, 3, 4, 5). The nstart option of this function can allow the algorithm to attempt multiple initial configurations and reports on the best one.

In order to view the results, swe can use fviz_cluster function which can provide a nice graph of the clusters. Usually, we have more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

```{r Cluster Analysis for Goalkeepers, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=20,fig.width=20}
gk2 <- kmeans(data_gk_int, centers = 2, nstart = 25)
gk3 <- kmeans(data_gk_int, centers = 3, nstart = 25)
gk4 <- kmeans(data_gk_int, centers = 4, nstart = 25)
gk5 <- kmeans(data_gk_int, centers = 5, nstart = 25)

# plots to compare
gkp1 <- fviz_cluster(gk2, geom = "point",  data = data_gk_int) + ggtitle("k = 2")
gkp2 <- fviz_cluster(gk3, geom = "point",  data = data_gk_int) + ggtitle("k = 3")
gkp3 <- fviz_cluster(gk4, geom = "point",  data = data_gk_int) + ggtitle("k = 4")
gkp4 <- fviz_cluster(gk5, geom = "point",  data = data_gk_int) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(gkp1, gkp2, gkp3, gkp4, nrow = 2)
```

<hr>

</br>

<h3> Determine the Number of Clusters </h3>

To determine the number of clusters, we use a simple within group sum of squares method. The idea is that, if the plot is an arm, the elbow of the arm is the optimal number of clusters, which is 3 in our case

```{r Determine the Number of Clusters GK, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
# Determine number of clusters
wss <- (nrow(data_gk_int)-1)*sum(apply(data_gk_int,2,var))

for (i in 2:12) wss[i] <- sum(kmeans(data_gk_int,
                                     centers=i)$withinss)
plot(1:12, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")

```

<hr>

</br>

<h3> Plotting the Clusters for Goalkeepers </h3>

```{r Plotting the GK cluster, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=6, fig.width= 6  }
library(fpc)
plotcluster(data_gk_int, gk3$cluster)
```

<hr />
</br>

<h3> <b> Checking the Prediction Strength - Goalkeepers </b> </h3>

Next we use the prediction strength with a cutoff value of 0.8. This will recommends the largest number of clusters that leads to a prediction strength above 0.8 which is again 3 in our case.

```{r Prediction Strength GK, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
prediction.strength(data_gk_int, Gmin=2, Gmax=15, M=10,cutoff=0.8)
```

<hr>

</br>

<h3> <b> Output of the K-Means - Goalkeepers </b> </h3>

```{r Output of the K-Means Goalkeeper, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
#use the output of kmeans
gk3$centers
```

<hr>

</br>

</br>

<h2> <b> Cluster Analysis for Outfield Players </b> </h2>

Next we perform cluster analysis of outfield players using kmeans() function and centers value of 2, 3, 4 and 5,.

```{r Cluster Analysis for Outflied Players, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=20,fig.width=20}
ok2 <- kmeans(data_outfield_int, centers = 2, nstart = 25)
ok3 <- kmeans(data_outfield_int, centers = 3, nstart = 25)
ok4 <- kmeans(data_outfield_int, centers = 4, nstart = 25)
ok5 <- kmeans(data_outfield_int, centers = 5, nstart = 25)

# plots to compare
op1 <- fviz_cluster(ok2, geom = "point",  data = data_outfield_int) + ggtitle("k = 2")
op2 <- fviz_cluster(ok3, geom = "point",  data = data_outfield_int) + ggtitle("k = 3")
op3 <- fviz_cluster(ok4, geom = "point",  data = data_outfield_int) + ggtitle("k = 4")
op4 <- fviz_cluster(ok5, geom = "point",  data = data_outfield_int) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(op1, op2, op3, op4, nrow = 2)
```

<hr>

</br>

<h3> <b> Determine the Number of Clusters - Outfield Players </b> </h3>

Using the sum of squares method method, we find the optimal number of clusters to be 3.

```{r Determine the Number of Clusters Outfield Players, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center' }
# Determine number of clusters
wss_o <- (nrow(data_outfield_int)-1)*sum(apply(data_outfield_int,2,var))

for (i in 2:12) wss_o[i] <- sum(kmeans(data_outfield_int,
                                     centers=i)$withinss)
plot(1:12, wss_o, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")

```

<hr>

</br>

<h3> Plotting the Clusters for Outfield Players </h3>

```{r Plotting the Outflied cluster, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=6, fig.width= 6  }
library(fpc)
plotcluster(data_outfield_int, ok3$cluster)
```

<hr />
</br>

<h3> <b> Checking the Prediction Strength - Outfield Players </b> </h3>

Using prediction strength function with a cut-off value of 0.8, we get the largest number of clusters to be 3.

```{r Prediction Strength Outfield, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center' }
prediction.strength(data_outfield_int, Gmin=2, Gmax=15, M=10,cutoff=0.8)
```

<hr>

</br>

<h3> <b> Output of the K-Means - Outfield Players </b> </h3>

```{r Output of the K-Means Outfield Players, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
#use the output of kmeans
ok3$centers
```


<hr>

</br>

### **Hierarchical Clustering**

<h3> Definition  </h3>
In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. 

Strategies for hierarchical clustering generally fall into two types:

1. Agglomerative: This is a "bottom-up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
2. Divisive: This is a "top-down" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.

In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering[2] are usually presented in a dendrogram.

</br>

More Information can be found on [**Wikipedia**](https://en.wikipedia.org/wiki/Hierarchical_clustering)

<hr>

</br>


<h2> Hierarchical Clustering Analysis for Goalkeepers </h2>

```{r Hierarchical Clustering Analysis for Goalkeepers, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=20,fig.width=20}
#Wards Method or Hierarchical clustering
#Calculate the distance matrix
gk.dist = dist(data_gk_int)
#Obtain clusters using the Wards method
gk.hclust = hclust(gk.dist, method = "ward")

plot(gk.hclust)

rect.hclust(hclust(gk.dist, method = "ward"), h = 500)

```


<hr>

</br>

<h3> Views the items in the selected cluster - Goalkeepers </h3>

```{r Items in the selected cluster GK, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.align='center', fig.height=10,fig.width=10  }

#Cut dendrogram at the 3 clusters level and obtain cluster membership
gk.3clust = cutree(gk.hclust,k=3)

#See exactly which item are in third group
data_gk_int[gk.3clust==3,]

```

<hr />
</br>

<h3> Plotting the hclust() for Goalkeepers (k=3) </h3>

```{r Plotting the hclust() for Goalkeepers  k 3, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=6, fig.width= 6 }
#get cluster means for raw data
#Centroid Plot against 1st 2 discriminant functions
#Load the fpc library needed for plotcluster function
library(fpc)
#plotcluster(ZooFood, fit$cluster)
plotcluster(data_gk_int, gk.3clust)
```

<hr/>
</br>



<h2> Hierarchical Clustering Analysis for Outfield Players</h2>

```{r Hierarchical Clustering Analysis for Outfield Players, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=20,fig.width=20}
#Wards Method or Hierarchical clustering
#Calculate the distance matrix
outfield.dist = dist(data_outfield_int)
#Obtain clusters using the Wards method
outfield.hclust = hclust(outfield.dist, method = "ward")

plot(outfield.hclust)


```


<h3> Views the items in the selected cluster - Outfield Players </h3>

```{r Items in the selected cluster Outfield players, results='hide', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center' }

#Cut dendrogram at the 3 clusters level and obtain cluster membership
outfield.3clust = cutree(outfield.hclust,k=3)

#See exactly which item are in third group
data_outfield_int[outfield.3clust==3,]

```

<hr />
</br>

<h3> Plotting the hclust() for Outfield Players (k=3) </h3>

```{r Plotting the hclust() for Outfield Players k 3, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=6, fig.width= 6  }
#get cluster means for raw data
#Centroid Plot against 1st 2 discriminant functions
#Load the fpc library needed for plotcluster function
library(fpc)
#plotcluster(ZooFood, fit$cluster)
plotcluster(data_outfield_int, outfield.3clust)
```

<hr/>
</br>




## **Comparison**


<h3> Hierarchical Clustering vs K-Means Clustering </h3>

Hierarchical clustering builds clusters within clusters and does not require a pre-specified number of clusters like k means. Now we discuss this difference in details. The most important difference is the hierarchy. Actually, there are two different approaches that fall under this name: top-down and bottom-up.

In top-down hierarchical clustering, we divide the data into 2 clusters (using k-means with k=2k=2, for example). Then, for each cluster, we can repeat this process, until all the clusters are too small or too similar for further clustering to make sense, or until we reach a preset number of clusters.

In bottom-up hierarchical clustering, we start with each data item having its own cluster. We then look for the two items that are most similar and combine them in a larger cluster. We keep repeating until all the clusters we have left are too dissimilar to be gathered together, or until we reach a preset number of clusters.

In k-means clustering, we try to identify the best way to divide the data into k sets simultaneously. A good approach is to take k items from the data set as initial cluster representatives, assign all items to the cluster whose representative is closest, and then calculate the cluster mean as a new representative; until it converges (all clusters stay the same)

Reference: [**StepupAnalytics.com**](https://stepupanalytics.com/difference-between-k-means-clustering-and-hierarchical-clustering/)

<hr>

</br>


<h3> Comparing kmeans() and hclust() </h3>

<h3> Goalkeepers </h3>

```{r Comparing kmeans and hclust goalkeepers, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'  }

# Apply cutree() to gk.hclust : gk.cut
gk.cut <- cutree(gk.hclust , k = 3)

# Compare methods
table(gk3$cluster, gk.cut)

```



</br>

<h3> Outfield Players </h3>

```{r Comparing kmeans and hclust outfield players, results='markup', echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}

# Apply cutree() to gk.hclust : gk.cut
outfield.cut <- cutree(outfield.hclust  , k = 3)

# Compare methods
table(ok3$cluster, outfield.cut)

```


</br>

## **Conclusion**

<h3> Summary of The Problem </h3>

In professional football, it is not uncommon for a player to drop out of team due to contract transfer, contract expiration or medical reasons. By the means of our analysis, we tried to find suitable replacements players based on their skillset, strength and physical attributes. We performed Clustering Analysis which, an unsupervised learning method and built two different models using K-means clustering and Hierarchical clustering and in the end compared both the models to see which one performs better.


</br>

<h3> Methodology </h3>

The methodology involved cleaning the dataset and then splitting it in 2 parts -  and Outfield Players. And then by only taking the physical attributes into consideration , performing clustering analysis - K mean Cluster Analysis and Hierarchical Analysis on both of the datasets separately to cluster similar physical type of player together and making it easier for the team management to find a suitable replacement in case of contract transfer, contract expiration or medical reasons.



</br>


<h3> Implication of the Analysis </h3>

This will be useful for team management as it will narrow down their search for a suitable replacement.




</br>
